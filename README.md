# Emoji_Prediction

## Goal
Given a sentence, predict one of five emojis  â¤ï¸ ðŸ˜‚ ðŸ˜ ðŸ˜¢ ðŸ˜¡.

## Dataset 
Preâ€‘labelled sentences (train/test split), also downloaded glove file form link-->  http://nlp.stanford.edu/data/glove.6B.zip

## Key Concepts
Tokenisation, Embedding layer, Biâ€‘LSTM, Softmax.


## Architecture
### Embedding Layer â€“ learns 128â€‘dimensional word vectors.
### Biâ€‘Directional LSTM â€“ hidden sizeâ€¯=â€¯256, 2 layers, dropoutâ€¯=â€¯0.3.
### Fully Connected + Softmax â€“ outputs 5â€‘class probabilities.

Accuracy = 87â€¯%


## Lessons Learned
Handsâ€‘on practice with tokenisers, padding, and packed sequences in PyTorch.
Saw how bidirectional context improves emoji recall for longer sentences.
Explored class imbalance mitigation with weighted loss.


