# Emoji_Prediction
Created an emoji prediction model using RNNs â€” feeding in text and letting the model decide the best-fitting emoji.

# Goal
Given a sentence, predict one of five emojis  â¤ï¸ ðŸ˜‚ ðŸ˜ ðŸ˜¢ ðŸ˜¡.

# Dataset 
Preâ€‘labelled sentences (train/test split).

### Framework --> PyTorchÂ 2.x

## Key Concepts
Tokenisation, Embedding layer, Biâ€‘LSTM, Softmax.


## Architecture
### Embedding Layer â€“ learns 128â€‘dimensional word vectors.
### Biâ€‘Directional LSTM â€“ hidden sizeâ€¯=â€¯256, 2 layers, dropoutâ€¯=â€¯0.3.
### Fully Connected + Softmax â€“ outputs 5â€‘class probabilities.

Accuracy = 87â€¯%


## Lessons Learned
Handsâ€‘on practice with tokenisers, padding, and packed sequences in PyTorch.
Saw how bidirectional context improves emoji recall for longer sentences.
Explored class imbalance mitigation with weighted loss.


